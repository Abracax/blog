<!DOCTYPE html><html lang="en"><head><title>Abracax</title><link rel="alternate" href="/atom.xml" type="application/atom.xml"><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"><link href="https://fonts.googleapis.com/css?family=Playfair+Display" rel="stylesheet"><script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"></script><script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"></script><meta charset="UTF-8"><link rel="icon" href="/images/favicon.png"><link rel="stylesheet" href="/css/style.css"><link rel="alternate" href="/atom.xml" title="Abracax's Notes" type="application/atom+xml">
</head><body><div class="site-header"><div class="container"><div class="site-branding"><h1 class="site-title">Abracax's Notes<a href="google.com"></a></h1><div class="site-description"><span class="site-description-text"></span></div></div></div></div><div class="menu_box"><div class="menucon"><nav class="navbar navbar-expand-lg navbar-light bg-light"><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><div class="menucon"><ul class="navbar-nav mr-auto"><li class="nav-item active"><a class="nav-link" href="/">Home</a></li></ul></div><div class="menucon"><ul class="navbar-nav mr-auto"><li class="nav-item active"><a class="nav-link" href="/resume">CV</a></li></ul></div><div class="menucon"><ul class="navbar-nav mr-auto"><li class="nav-item active"><a class="nav-link" target="_blank" rel="noopener" href="https://github.com/abracax">GitHub</a></li></ul></div><div class="menucon"><ul class="navbar-nav mr-auto"><li class="nav-item active"><a class="nav-link" href="/archives/">Archive</a></li></ul></div><div class="menucon"><ul class="navbar-nav mr-auto"><li class="nav-item active"><a class="nav-link" href="/test/">Test</a></li></ul></div></div></nav></div></div></body><div class="container"><div class="row"><div class="single-post"><article class="post-block"><h1 class="post-title">CUDA ECE 408</h1><header class="headgallery"></header><div class="row"><div class="tag-box"><div class="tags"></div></div></div><div class="post-content"><h2 id="Sparse-Matrix-Vector-Multiplication"><a href="#Sparse-Matrix-Vector-Multiplication" class="headerlink" title="Sparse Matrix-Vector Multiplication"></a>Sparse Matrix-Vector Multiplication</h2><p><strong>M rows, N columns, K non-zeros, L non-zero elements in the row with the largest number of non-zeros.</strong></p>
<p>COO: 3K</p>
<p>CSR: 2K + M + 1</p>
<p>ELL: 2M*L</p>
<p>JDS: 2K+2M</p>
<p>JDS-T: 2K + 2M + L</p>
<p>Aligned accesses need to start at a 16-word boundary and threads in a warp need to access continuous locations. Non-aligned need not start at a 16-word boundary. Whereas coalesced just means that threads in a warp need to access continuous locations. Transposition helps reduce non-coalesced accesses.</p>
<p><strong>ELL</strong> best for control divergence.</p>
<p><strong>ELL</strong> and <strong>JDS-T</strong> coalesced</p>
<p><img src="/2020/11/15/CUDA/image-20201209004919794.png" alt="image-20201209004919794"></p>
<p><img src="/2020/11/15/CUDA/image-20201209004948402.png" alt="image-20201209004948402"></p>
<p><img src="/2020/11/15/CUDA/image-20201209005013410.png" alt="image-20201209005013410"></p>
<p><img src="/2020/11/15/CUDA/image-20201209005036563.png" alt="image-20201209005036563"></p>
<h2 id="Kogge-Stone-scan-kernel"><a href="#Kogge-Stone-scan-kernel" class="headerlink" title="Kogge-Stone scan kernel"></a>Kogge-Stone scan kernel</h2><p>Total # of add operations: n * log(n) - (n-1)</p>
<h3 id="Control-Divergence"><a href="#Control-Divergence" class="headerlink" title="Control Divergence"></a>Control Divergence</h3><p>1024 elements  where stride is 16: 1 warp</p>
<p>1024 elements  where stride is 64: 0 warp</p>
<h2 id="Brent-Kung-Scan-kernel"><a href="#Brent-Kung-Scan-kernel" class="headerlink" title="Brent-Kung Scan kernel"></a>Brent-Kung Scan kernel</h2><p>Total # of add operations:  2*(n-1) – log(n)</p>
<h2 id="Privatisation-and-Histogramming"><a href="#Privatisation-and-Histogramming" class="headerlink" title="Privatisation and Histogramming"></a>Privatisation and Histogramming</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">histo_kernel</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">char</span> *buffer, <span class="keyword">long</span> size, <span class="keyword">unsigned</span> <span class="keyword">int</span> *histo)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> __shared__ <span class="keyword">unsigned</span> <span class="keyword">int</span> histo_private[<span class="number">1024</span>];</span><br><span class="line"></span><br><span class="line"> <span class="keyword">for</span> (<span class="keyword">int</span> i = threadIdx.x; i &lt; <span class="number">1024</span>; i+=blockDim.x) histo_privat[i] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"> __syncthreads();</span><br><span class="line"> <span class="keyword">int</span> i = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line"></span><br><span class="line"><span class="comment">// stride is total number of threads </span></span><br><span class="line"><span class="keyword">int</span> stride = blockDim.x * gridDim.x</span><br><span class="line"><span class="keyword">while</span> (i &lt; size) &#123;</span><br><span class="line">  <span class="keyword">if</span> ( buffer[i]... ) atomicAdd( &amp;(private_histo[buffer[i]), <span class="number">1</span>); </span><br><span class="line">  <span class="keyword">else</span> atomicAdd( &amp;(histo[buffer[i]), <span class="number">1</span> );</span><br><span class="line">  i += stride; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__syncthreads();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i= <span class="number">0</span>; i &lt; <span class="number">2048</span>; i += <span class="number">1</span>)</span><br><span class="line">    atomicAdd( &amp;(histo[buffer[i]),  private_histo[buffer[i]); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Theoretical-minimum-runtime-of-the-original-histogram-kernel"><a href="#Theoretical-minimum-runtime-of-the-original-histogram-kernel" class="headerlink" title="Theoretical minimum runtime of the original histogram kernel"></a>Theoretical minimum runtime of the <strong>original histogram kernel</strong></h4><ol>
<li><p>(blockDim*loop_num) *largest_percentage * 1 ns + gridDim * 100 ns</p>
<p>Loop_num is also number of elements per thread.</p>
</li>
</ol>
<p><img src="/2020/11/15/CUDA/image-20201208224920842.png" alt="image-20201208224920842"></p>
<p>10**7 + 5000</p>
<h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>dim3 outputDim(W_out, H_out, 1);</p>
<p>dim3 inputDim(W, H, 1);</p>
<ol>
<li>OutputDim maps to the output so it will use fewer threads, this also has less control divergence if using shared memory and strategy 2. inputDim maps to the input so it can load the shared memory in a single pass.</li>
</ol>
<h2 id="Convolution-calculation"><a href="#Convolution-calculation" class="headerlink" title="Convolution calculation"></a>Convolution calculation</h2><p>For input elements</p>
<p>– Each output tile has TILE_WIDTH^2 elements<br> – Each input tile has (TILE_WIDTH+K-1)^2<br> – The total number of input feature map element accesses was TILE_WIDTH^2 * K^2<br> – The reduction factor of the tiled algorithm is K^2 * TILE_WIDTH^2 / (TILE_WIDTH+K-1)^2</p>
<h2 id="Unroll"><a href="#Unroll" class="headerlink" title="Unroll"></a>Unroll</h2><p><img src="/2020/11/15/CUDA/image-20201209094952824.png" alt="image-20201209094952824"></p>
<p><img src="/2020/11/15/CUDA/image-20201209095014432.png" alt="image-20201209095014432"></p>
</div></article><div class="paginator"><a class="prev" href="/2021/04/14/ARIES/">prev</a><a class="next" href="/2020/07/06/EPOLL/">next</a></div></div></div></div><div class="container-fluid"><div class="row"><div class="footer_box"><a class="col-lg-2 col-xl-2 col-md-2" href="/resume">Contact</a></div></div></div></html>